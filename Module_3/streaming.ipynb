{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Conversation\n",
    "def model_call_node(state: State):\n",
    "\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    if summary:\n",
    "        system_message = f\"Summary of the previous conversation: {summary}\"\n",
    "\n",
    "        messages = [SystemMessage(content=system_message)] + state['messages']\n",
    "\n",
    "    else:\n",
    "        messages = state['messages']\n",
    "        # system_message = \"Summarize the conversation on few lines.\"\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    summary = state.get('summary', '')\n",
    "    if summary:\n",
    "        message_summary = (\n",
    "            f\"This is the summary of the previous conversation: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above\"\n",
    "        )\n",
    "    else:\n",
    "        message_summary = \"Create a summary of the recent messgges.\"\n",
    "\n",
    "    messages  = state[\"messages\"] + [HumanMessage(content = message_summary)]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    delete_messages = [RemoveMessage(m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages)>4:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x20698a03d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"model_call_node\", model_call_node)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x20698a03d90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edges\n",
    "workflow.add_edge(START, \"model_call_node\")\n",
    "workflow.add_conditional_edges(\"model_call_node\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_call_node': {'messages': AIMessage(content='Hello, Narcyy! It seems like you’re introducing yourself again. Is there something specific you’d like to talk about or ask?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 145, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-e60b5f52-6647-430d-ba92-ae7e08ba3441-0', usage_metadata={'input_tokens': 145, 'output_tokens': 29, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
      "{'summarize_conversation': {'summary': \"Sure! Here’s an extended summary of our conversation:\\n\\n1. You introduced yourself as Narcyy multiple times.\\n2. I acknowledged your introduction and asked how I could assist you.\\n3. You repeated your introduction again, prompting me to inquire if there was something specific you wanted to discuss or ask.\\n\\nIf there's anything else you'd like to add or if you have further questions, just let me know!\", 'messages': [RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='95327a99-acb7-4e60-806b-ca31a0e480cf'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-d4e79450-6bf7-4933-9749-938fba6725f6-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='35a7e228-836b-41f5-a674-73c26eeee5da'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-9b46cfda-2c19-4d1f-952c-5aaf3a38068e-0')]}}\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"Hi,I am Narcyy\")\n",
    "async for chunk in graph.astream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hi,I am Narcyy', additional_kwargs={}, response_metadata={}, id='95327a99-acb7-4e60-806b-ca31a0e480cf'), AIMessage(content='Hello, Narcyy! It’s nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 108, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d4e79450-6bf7-4933-9749-938fba6725f6-0', usage_metadata={'input_tokens': 108, 'output_tokens': 21, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Hi,I am Narcyy', additional_kwargs={}, response_metadata={}, id='35a7e228-836b-41f5-a674-73c26eeee5da')], 'summary': 'Certainly! Here’s a summary of our recent messages:\\n\\n1. You introduced yourself as Narcyy multiple times.\\n2. I acknowledged your introduction and asked how I could assist you.\\n\\nIf there’s anything specific you’d like to discuss or if you have questions, feel free to let me know!'}\n",
      "{'messages': [HumanMessage(content='Hi,I am Narcyy', additional_kwargs={}, response_metadata={}, id='95327a99-acb7-4e60-806b-ca31a0e480cf'), AIMessage(content='Hello, Narcyy! It’s nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 108, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d4e79450-6bf7-4933-9749-938fba6725f6-0', usage_metadata={'input_tokens': 108, 'output_tokens': 21, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Hi,I am Narcyy', additional_kwargs={}, response_metadata={}, id='35a7e228-836b-41f5-a674-73c26eeee5da'), AIMessage(content='Hi again, Narcyy! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 117, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9b46cfda-2c19-4d1f-952c-5aaf3a38068e-0', usage_metadata={'input_tokens': 117, 'output_tokens': 15, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'summary': 'Certainly! Here’s a summary of our recent messages:\\n\\n1. You introduced yourself as Narcyy multiple times.\\n2. I acknowledged your introduction and asked how I could assist you.\\n\\nIf there’s anything specific you’d like to discuss or if you have questions, feel free to let me know!'}\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"Hi,I am Narcyy\")\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
